{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import keras\n",
    "\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"input/train.csv\")\n",
    "test = pd.read_csv(\"input/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, we are just setting the Y and X from the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train[\"label\"]\n",
    "\n",
    "# Drop 'label' column\n",
    "X_train = train.drop(labels = [\"label\"],axis = 1) \n",
    "\n",
    "sns.countplot(Y_train)\n",
    "\n",
    "Y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing GrayScale Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "test = test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "test = test.values.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = to_categorical(Y_train, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plt.imshow(X_train[0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building my own model + Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation is the process of changing or adding noise to image. This could be rotating, or shifting the images. Essentially adding noise allows for the data to be better prepared. We have to be careful with some image augmentation process such as rotation as this can cause confusion between the numbers 6 and 9. Here context means a lot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to create augmentation of images\n",
    "def augmentImages(images, multiple):\n",
    "    gen = ImageDataGenerator()\n",
    "    newImages = []\n",
    "    for _ in range(multiple):\n",
    "        for image in images:\n",
    "            transformedImage = gen.apply_transform(image, {'brightness': np.random.uniform(low = 0.5, high=0.99),\n",
    "                                                            'tx': np.random.random_integers(2),\n",
    "                                                            'ty': np.random.random_integers(2)})\n",
    "            newImages.append(transformedImage)\n",
    "    newImages = np.array(newImages)\n",
    "    return newImages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In special cases such as image detection we have a base layer, (which soles purpose is to detect features in an image). These are the layers with Conv2D and MaxPoooling2D. These layers extract certain features from the data.\n",
    "\n",
    "#### The convolution filters an image. The activation relu is used to detect that feature with in the filtered image. Finally, maxpooling condenses the image layer to enhance the features. \n",
    "\n",
    "#### The 3 step as summarized are:\n",
    "\n",
    "##### 1. Conv2D -> Convolutions (Filters the image)\n",
    "\n",
    "##### 2. Relu -> Detect feature\n",
    "\n",
    "##### 3. MaxPooling2D -> Enhances the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this project I'm going to experiment with different model to see how performance would vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Base: This is where we apply the 3 feature detection steps above\n",
    "model.add(Conv2D(16, (3,3), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(64, (2,2), activation='relu'))\n",
    "model.add(Conv2D(128,(2,2), activation ='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Head: Thisi s where we identify the class of the image\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "# Base: This is where we apply the 3 feature detection steps above\n",
    "model2.add(Conv2D(16, (3,3), activation='relu', input_shape=(28,28,1)))\n",
    "model2.add(Conv2D(32, (3,3), activation='relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model2.add(Dropout(0.3))\n",
    "\n",
    "# Head: Thisi s where we identify the class of the image\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "# Base: This is where we apply the 3 feature detection steps above\n",
    "model3.add(Conv2D(8, (2,2), activation='relu', input_shape=(28,28,1)))\n",
    "model3.add(Conv2D(16, (2,2), activation='relu'))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model3.add(Dropout(0.3))\n",
    "\n",
    "model3.add(Conv2D(32, (2,2), activation='relu'))\n",
    "model3.add(Conv2D(64,(2,2), activation ='relu'))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model3.add(Dropout(0.3))\n",
    "\n",
    "# Head: Thisi s where we identify the class of the image\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(100, activation='relu'))\n",
    "model3.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After defining the model, the next step is to compile and define optimizers. These include the loss functions and optimizers. The loss function defines how the model should be trained. The optimizers are the ways the model will use to improve it's loss function. We also have metrics = ['accuracy] which essentially is a parameters that tells the model to print the accuracy every epoch iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(1e-3),\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "model2.compile(optimizer=keras.optimizers.SGD(1e-3),\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "model3.compile(optimizer=keras.optimizers.SGD(1e-3),\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newNumbers = augmentImages([X_train[0]],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(newNumbers.reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentPic = augmentImages(X_train,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffling the data is always good practice! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = shuffle(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size = 32, epochs=50, validation_split=0.4, callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model2.fit(x_train, y_train, batch_size = 32, epochs=50, validation_split=0.4, callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.fit(x_train, y_train, batch_size = 32, epochs=50, validation_split=0.4, callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It's interesting to see that the second model only took 34 epochs compared to the first and third model. This model has more layers instead of more neurons in a single layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, for each model we are going to see the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_val, Y_val, verbose = 0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model2.evaluate(X_val, Y_val, verbose = 0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model3.evaluate(X_val, Y_val, verbose = 0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
